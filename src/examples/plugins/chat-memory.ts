/**
 * @file Chat Memory Plugin
 * @description A plugin that saves tool responses to an in-memory database and allows 
 * the calling LLM to chat with saved results through an interpreter LLM
 */

import { BasePlugin, PluginContext, PluginConfig } from '../../interfaces/plugin.js';
import { ToolCallResult } from '../../interfaces/proxy-hooks.js';

/**
 * Stored conversation entry
 */
interface ConversationEntry {
  id: string;
  toolName: string;
  request: {
    args: Record<string, any>;
    timestamp: number;
  };
  response: {
    content: string;
    metadata?: Record<string, any>;
    timestamp: number;
  };
  context: {
    requestId: string;
    userId?: string;
    sessionId?: string;
  };
}

/**
 * Chat session for interacting with saved data
 */
interface ChatSession {
  id: string;
  userId?: string;
  messages: ChatMessage[];
  createdAt: number;
  lastActivity: number;
}

/**
 * Chat message in a session
 */
interface ChatMessage {
  id: string;
  type: 'user' | 'assistant' | 'system';
  content: string;
  timestamp: number;
  metadata?: Record<string, any>;
}

/**
 * LLM provider interface for chat interpretation
 */
interface ChatLLMProvider {
  generateResponse(
    conversation: ConversationEntry[],
    userMessage: string,
    context: ChatContext
  ): Promise<string>;
}

/**
 * Context for chat LLM
 */
interface ChatContext {
  sessionId: string;
  userId?: string;
  availableEntries: ConversationEntry[];
  chatHistory: ChatMessage[];
}

/**
 * Mock chat LLM provider for testing
 */
class MockChatLLMProvider implements ChatLLMProvider {
  private delay: number;

  constructor(delay = 100) {
    this.delay = delay;
  }

  async generateResponse(
    conversation: ConversationEntry[],
    userMessage: string,
    context: ChatContext
  ): Promise<string> {
    await new Promise(resolve => setTimeout(resolve, this.delay));

    // Simple mock response based on user message
    const entryCount = conversation.length;
    const recentEntry = conversation[conversation.length - 1];
    
    if (userMessage.toLowerCase().includes('what') || userMessage.toLowerCase().includes('show')) {
      return `Based on your ${entryCount} saved conversations, here's what I found: ${recentEntry?.response.content.substring(0, 100)}... Would you like me to analyze any specific entries?`;
    }
    
    if (userMessage.toLowerCase().includes('analyze') || userMessage.toLowerCase().includes('summary')) {
      return `I've analyzed your ${entryCount} conversations. The most recent was from tool "${recentEntry?.toolName}" which returned information about ${JSON.stringify(recentEntry?.request.args)}. The results show ${recentEntry?.response.content.length} characters of data.`;
    }
    
    if (userMessage.toLowerCase().includes('search') || userMessage.toLowerCase().includes('find')) {
      const relevantEntries = conversation.filter(entry => 
        entry.response.content.toLowerCase().includes(userMessage.toLowerCase().split(' ')[1] || '')
      );
      return `Found ${relevantEntries.length} relevant entries that match your search. ${relevantEntries.length > 0 ? `The first match is from "${relevantEntries[0].toolName}" tool.` : ''}`;
    }

    return `I understand you're asking: "${userMessage}". I have access to ${entryCount} saved conversations. What specific information would you like me to help you find or analyze?`;
  }
}

/**
 * OpenAI chat provider for production use
 */
class OpenAIChatProvider implements ChatLLMProvider {
  private apiKey: string;
  private model: string;

  constructor(apiKey: string, model = 'gpt-4o-mini') {
    this.apiKey = apiKey;
    this.model = model;
  }

  async generateResponse(
    conversation: ConversationEntry[],
    userMessage: string,
    context: ChatContext
  ): Promise<string> {
    if (!this.apiKey) {
      throw new Error('OpenAI API key not provided');
    }

    // Mock implementation - in reality this would call OpenAI API
    const contextSummary = this.buildContextSummary(conversation);
    return `AI Response based on ${conversation.length} saved entries: ${contextSummary}. User asked: "${userMessage}". [This would be generated by ${this.model}]`;
  }

  private buildContextSummary(conversation: ConversationEntry[]): string {
    const tools = [...new Set(conversation.map(e => e.toolName))];
    const totalResponses = conversation.length;
    const averageResponseLength = conversation.reduce((sum, e) => sum + e.response.content.length, 0) / totalResponses;
    
    return `${totalResponses} responses from tools: ${tools.join(', ')}. Average response length: ${Math.round(averageResponseLength)} chars`;
  }
}

/**
 * Chat Memory Plugin
 * Saves tool responses to memory and provides chat interface for interacting with saved data
 */
export class ChatMemoryPlugin extends BasePlugin {
  name = 'chat-memory-plugin';
  version = '1.0.0';

  metadata = {
    description: 'Saves tool responses to memory and provides chat interface for interacting with saved data',
    author: 'MCP Team',
    tags: ['memory', 'chat', 'ai', 'database']
  };

  config: PluginConfig = {
    enabled: true,
    priority: 20, // Run after other plugins
    options: {
      provider: 'mock', // 'mock' or 'openai'
      openaiApiKey: process.env.OPENAI_API_KEY,
      model: 'gpt-4o-mini',
      saveResponses: true,
      enableChat: true,
      maxEntries: 1000,
      maxSessions: 100,
      sessionTimeout: 24 * 60 * 60 * 1000, // 24 hours
      saveTools: [], // Empty = save all tools
      excludeTools: ['chat-with-memory', 'get-memory-stats'], // Don't save these
      mockDelay: 100
    }
  };

  private chatProvider: ChatLLMProvider;
  
  // In-memory storage
  private conversationDb = new Map<string, ConversationEntry>();
  private chatSessions = new Map<string, ChatSession>();
  
  // Statistics
  private stats = {
    totalEntries: 0,
    totalSessions: 0,
    totalChatMessages: 0,
    storageSize: 0
  };

  async initialize(context: any): Promise<void> {
    await super.initialize(context);

    // Initialize chat LLM provider
    const provider = this.config.options?.provider || 'mock';
    
    if (provider === 'openai') {
      this.chatProvider = new OpenAIChatProvider(
        this.config.options?.openaiApiKey,
        this.config.options?.model
      );
    } else {
      this.chatProvider = new MockChatLLMProvider(this.config.options?.mockDelay);
    }

    this.logger?.info(`Chat Memory plugin initialized with ${provider} provider`);
    
    // Start cleanup interval
    this.startCleanupInterval();
  }

  async afterToolCall(context: PluginContext, result: ToolCallResult): Promise<ToolCallResult> {
    // Check if we should save this tool response
    if (!this.shouldSave(context, result)) {
      return result;
    }

    try {
      // Extract content from result
      const content = this.extractContent(result);
      
      if (content.length === 0) {
        return result;
      }

      // Create conversation entry
      const entry: ConversationEntry = {
        id: this.generateEntryId(),
        toolName: context.toolName,
        request: {
          args: context.args,
          timestamp: context.startTime || Date.now()
        },
        response: {
          content,
          metadata: result.result._metadata,
          timestamp: Date.now()
        },
        context: {
          requestId: context.requestId,
          userId: context.args.userId as string,
          sessionId: context.args.sessionId as string
        }
      };

      // Save to database
      await this.saveEntry(entry);

      // Add memory metadata to result
      const enhancedResult: ToolCallResult = {
        ...result,
        result: {
          ...result.result,
          _metadata: {
            ...result.result._metadata,
            savedToMemory: true,
            memoryId: entry.id,
            chatAvailable: this.config.options?.enableChat,
            memoryStats: {
              totalEntries: this.stats.totalEntries,
              storageSize: this.stats.storageSize
            }
          }
        }
      };

      this.logger?.debug(`Saved conversation entry for ${context.toolName}`, {
        entryId: entry.id,
        contentLength: content.length
      });

      return enhancedResult;

    } catch (error) {
      this.logger?.error(`Failed to save conversation entry for ${context.toolName}:`, error);
      // Return original result if saving fails
      return result;
    }
  }

  private shouldSave(context: PluginContext, result: ToolCallResult): boolean {
    // Don't save if disabled
    if (!this.config.options?.saveResponses) {
      return false;
    }

    // Don't save errors
    if (result.result.isError) {
      return false;
    }

    // Check tool exclusions
    const excludeTools = this.config.options?.excludeTools || [];
    if (excludeTools.includes(context.toolName)) {
      return false;
    }

    // Check tool inclusions (if specified)
    const saveTools = this.config.options?.saveTools || [];
    if (saveTools.length > 0 && !saveTools.includes(context.toolName)) {
      return false;
    }

    return true;
  }

  private extractContent(result: ToolCallResult): string {
    if (!result.result.content) return '';

    return result.result.content
      .filter((item: any) => item.type === 'text')
      .map((item: any) => item.text)
      .join('\n');
  }

  private generateEntryId(): string {
    return `entry_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private generateSessionId(): string {
    return `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private generateMessageId(): string {
    return `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private async saveEntry(entry: ConversationEntry): Promise<void> {
    this.conversationDb.set(entry.id, entry);
    this.stats.totalEntries++;
    this.stats.storageSize += JSON.stringify(entry).length;

    // Cleanup old entries if limit exceeded
    await this.cleanupEntries();
  }

  private async cleanupEntries(): Promise<void> {
    const maxEntries = this.config.options?.maxEntries || 1000;
    
    if (this.conversationDb.size > maxEntries) {
      const entries = Array.from(this.conversationDb.entries())
        .sort((a, b) => a[1].response.timestamp - b[1].response.timestamp);
      
      const toRemove = entries.slice(0, this.conversationDb.size - maxEntries);
      
      for (const [id] of toRemove) {
        this.conversationDb.delete(id);
      }
      
      this.logger?.debug(`Cleaned up ${toRemove.length} old conversation entries`);
    }
  }

  private startCleanupInterval(): void {
    setInterval(() => {
      this.cleanupSessions();
    }, 60 * 60 * 1000); // Run every hour
  }

  private cleanupSessions(): void {
    const timeout = this.config.options?.sessionTimeout || 24 * 60 * 60 * 1000;
    const now = Date.now();
    
    for (const [sessionId, session] of this.chatSessions.entries()) {
      if (now - session.lastActivity > timeout) {
        this.chatSessions.delete(sessionId);
      }
    }
  }

  // Public methods for chat functionality

  /**
   * Start or continue a chat session
   */
  async startChatSession(userId?: string, sessionId?: string): Promise<string> {
    const id = sessionId || this.generateSessionId();
    
    if (!this.chatSessions.has(id)) {
      const session: ChatSession = {
        id,
        userId,
        messages: [],
        createdAt: Date.now(),
        lastActivity: Date.now()
      };
      
      this.chatSessions.set(id, session);
      this.stats.totalSessions++;
    }
    
    return id;
  }

  /**
   * Send a message in a chat session
   */
  async chatWithMemory(
    sessionId: string,
    userMessage: string,
    userId?: string
  ): Promise<string> {
    const session = this.chatSessions.get(sessionId);
    if (!session) {
      throw new Error(`Chat session ${sessionId} not found`);
    }

    // Add user message to session
    const userMsg: ChatMessage = {
      id: this.generateMessageId(),
      type: 'user',
      content: userMessage,
      timestamp: Date.now()
    };
    
    session.messages.push(userMsg);
    session.lastActivity = Date.now();

    // Get relevant conversation entries
    const relevantEntries = this.getRelevantEntries(userId, userMessage);
    
    // Build context for chat LLM
    const context: ChatContext = {
      sessionId,
      userId,
      availableEntries: relevantEntries,
      chatHistory: session.messages.slice(-10) // Last 10 messages
    };

    // Generate response using chat LLM
    const response = await this.chatProvider.generateResponse(
      relevantEntries,
      userMessage,
      context
    );

    // Add assistant response to session
    const assistantMsg: ChatMessage = {
      id: this.generateMessageId(),
      type: 'assistant',
      content: response,
      timestamp: Date.now(),
      metadata: {
        entriesConsidered: relevantEntries.length,
        provider: this.config.options?.provider
      }
    };
    
    session.messages.push(assistantMsg);
    this.stats.totalChatMessages += 2; // User + assistant

    return response;
  }

  private getRelevantEntries(userId?: string, query?: string): ConversationEntry[] {
    let entries = Array.from(this.conversationDb.values());

    // Filter by user if specified
    if (userId) {
      entries = entries.filter(entry => entry.context.userId === userId);
    }

    // Simple relevance filtering based on query
    if (query) {
      const queryLower = query.toLowerCase();
      entries = entries.filter(entry => 
        entry.toolName.toLowerCase().includes(queryLower) ||
        entry.response.content.toLowerCase().includes(queryLower) ||
        JSON.stringify(entry.request.args).toLowerCase().includes(queryLower)
      );
    }

    // Sort by timestamp (most recent first)
    entries.sort((a, b) => b.response.timestamp - a.response.timestamp);

    // Limit to most relevant entries
    return entries.slice(0, 20);
  }

  /**
   * Get conversation entries for a user
   */
  getConversationHistory(userId?: string, limit = 50): ConversationEntry[] {
    let entries = Array.from(this.conversationDb.values());
    
    if (userId) {
      entries = entries.filter(entry => entry.context.userId === userId);
    }
    
    entries.sort((a, b) => b.response.timestamp - a.response.timestamp);
    
    return entries.slice(0, limit);
  }

  /**
   * Search conversation entries
   */
  searchConversations(query: string, userId?: string): ConversationEntry[] {
    const queryLower = query.toLowerCase();
    let entries = Array.from(this.conversationDb.values());
    
    if (userId) {
      entries = entries.filter(entry => entry.context.userId === userId);
    }
    
    return entries.filter(entry =>
      entry.toolName.toLowerCase().includes(queryLower) ||
      entry.response.content.toLowerCase().includes(queryLower) ||
      JSON.stringify(entry.request.args).toLowerCase().includes(queryLower)
    );
  }

  /**
   * Get a specific conversation entry
   */
  getConversationEntry(entryId: string): ConversationEntry | null {
    return this.conversationDb.get(entryId) || null;
  }

  /**
   * Get chat session
   */
  getChatSession(sessionId: string): ChatSession | null {
    const session = this.chatSessions.get(sessionId);
    if (session) {
      session.lastActivity = Date.now();
    }
    return session || null;
  }

  /**
   * Clear all memory for a user
   */
  clearUserMemory(userId: string): number {
    let cleared = 0;
    
    for (const [id, entry] of this.conversationDb.entries()) {
      if (entry.context.userId === userId) {
        this.conversationDb.delete(id);
        cleared++;
      }
    }
    
    // Clear chat sessions for user
    for (const [sessionId, session] of this.chatSessions.entries()) {
      if (session.userId === userId) {
        this.chatSessions.delete(sessionId);
      }
    }
    
    return cleared;
  }

  /**
   * Get plugin statistics
   */
  async getStats() {
    const baseStats = await super.getStats();
    
    return {
      ...baseStats,
      customMetrics: {
        totalEntries: this.stats.totalEntries,
        totalSessions: this.stats.totalSessions,
        totalChatMessages: this.stats.totalChatMessages,
        storageSize: this.stats.storageSize,
        activeSessions: this.chatSessions.size,
        provider: this.config.options?.provider || 'mock',
        memoryUsage: `${Math.round(this.stats.storageSize / 1024)} KB`,
        averageEntrySize: Math.round(this.stats.storageSize / this.stats.totalEntries) || 0
      }
    };
  }

  async destroy(): Promise<void> {
    this.logger?.info('Chat Memory plugin shutting down');
    
    // Log final stats
    const stats = await this.getStats();
    this.logger?.info('Final plugin statistics:', stats.customMetrics);
    
    // Clear storage
    this.conversationDb.clear();
    this.chatSessions.clear();
  }
}

// Export types for testing
export { ConversationEntry, ChatSession, ChatMessage, ChatLLMProvider, MockChatLLMProvider, OpenAIChatProvider };